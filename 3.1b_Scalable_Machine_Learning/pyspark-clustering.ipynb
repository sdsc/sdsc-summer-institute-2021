{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Cluster Analysis on Weather Data\n",
    "\n",
    "## CIML Summer Institute\n",
    "\n",
    "Mai H. Nguyen, UC San Diego\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.SparkConf().setAll([('spark.master', 'local[2]'),\n",
    "                                   ('spark.app.name', 'PySpark Cluster Analysis')])\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "print (spark.version)\n",
    "print (pyspark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Show plots in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source:  http://hpwren.ucsd.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField \n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Specify schema\n",
    "schema = StructType ([ \\\n",
    "    StructField (\"rowID\",IntegerType(),True), \\\n",
    "    StructField (\"hpwren_timestamp\",TimestampType(),True), \\\n",
    "    StructField (\"air_pressure\",DoubleType(),True), \\\n",
    "    StructField (\"air_temp\",DoubleType(),True), \\\n",
    "    StructField (\"avg_wind_direction\",DoubleType(),True), \\\n",
    "    StructField (\"avg_wind_speed\",DoubleType(),True), \\\n",
    "    StructField (\"max_wind_direction\",DoubleType(),True), \\\n",
    "    StructField (\"max_wind_speed\",DoubleType(),True), \\\n",
    "    StructField (\"min_wind_direction\",DoubleType(),True), \\\n",
    "    StructField (\"min_wind_speed\",DoubleType(),True), \\\n",
    "    StructField (\"rain_accumulation\",DoubleType(),True), \\\n",
    "    StructField (\"rain_duration\",DoubleType(),True), \\\n",
    "    StructField (\"relative_humidity\",DoubleType(),True)\n",
    "])\n",
    "\n",
    "# Read in data and put in Spark DataFrame\n",
    "import os\n",
    "home_dir = os.getenv('HOME')\n",
    "\n",
    "inputfile = home_dir + <<FILL-IN>>   # Read data from \"data/ml/minute_weather.csv\"\n",
    "df = spark.read.csv (inputfile, header=True, schema=schema).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<FILL-IN>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL-IN>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<FILL-IN>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingDF = <<FILL-IN>>   \n",
    "workingDF.<<FILL-IN>>             # Get count of rows after dropping null data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "featuresUsed = ['air_pressure', 'air_temp', 'avg_wind_direction', 'avg_wind_speed', 'max_wind_direction', \n",
    "        'max_wind_speed','relative_humidity']\n",
    "assembler = VectorAssembler(inputCols=featuresUsed, outputCol=\"features_unscaled\")\n",
    "assembled = assembler.transform(workingDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled.<<FILL-IN>>      # Show first row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "scalerModel = scaler.fit(assembled)\n",
    "scaledData = scalerModel.transform(assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<FILL-IN>>          # Show first row of scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform cluster analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate elbow plot to determine value(s) for k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import utils\n",
    "\n",
    "# Only need to run this once to find value(s) to try for k\n",
    "# Set to False if already know value for k\n",
    "create_elbow_plot = True \n",
    "\n",
    "# Get elbow plot using subset of data\n",
    "if create_elbow_plot == True:\n",
    "    sampledData = scaledData.filter((scaledData.rowID % 20) == 0).select(\"features\").cache() \n",
    "    k_attempts = range(5,15)\n",
    "    print('Trying k from {} to {} with {} samples\\n'.format(list(k_attempts)[0],\n",
    "                                                          list(k_attempts)[-1], \n",
    "                                                          sampledData.count()))\n",
    "    wsseList = utils.elbow(sampledData, k_attempts)\n",
    "    utils.elbow_plot(wsseList, k_attempts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Clustering Using K-Means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Only the features vector is used as input to model\n",
    "scaledDataFeat = scaledData.select(<<FILL-IN>>).cache()\n",
    "\n",
    "# Set number of clusters\n",
    "nClusters = 11\n",
    "\n",
    "kmeans = KMeans(k=nClusters, seed=1)\n",
    "model = <<FILL-IN>>       # Fit model to scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show cluster centers\n",
    "\n",
    "centers = model.clusterCenters()\n",
    "pd.DataFrame(centers,columns=featuresUsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show cluster sizes\n",
    "\n",
    "model.<<FILL-IN>>     # Get cluster sizes from model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate cluster profile plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centersNamed = utils.pd_centers(featuresUsed,centers)\n",
    "print(centersNamed.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiles for All Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a parallel plot to show the cluster centers for all clusters.  \n",
    "Data samples were standardized to have 0-mean and 1-standard-deviation before k-means was applied, so values of cluster centers can be interpreted as standard deviations from the mean.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numClusters = len(centersNamed.index)\n",
    "colors_used = utils.parallel_plot(centersNamed, numClusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters Capturing Dry Days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters with lower-than-average relative_humidity values capture dry days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.parallel_plot(centersNamed[centersNamed['relative_humidity'] < -0.5], \n",
    "                   numClusters, colors=colors_used);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters Capturing Humid Days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters with higher-than-average relative_humidity values capture humid days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<FILL-IN>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters Capturing Hot Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<FILL-IN>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters Capturing Windy Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<FILL-IN>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(<<FILL-IN>>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<FILL-IN>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
